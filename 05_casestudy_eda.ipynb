{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Age in American Politics\n",
    "\n",
    "In the last few section workbooks, we've reviewed Python basics, used a little `pandas`, wrangled and cleaned data, and carried out an exploratory data analysis where we generated a number of basic visualizations. This week, we'll extend this a bit futher, incorporating all that we've learned so far AND considering what we discussed in lecture last week. \n",
    "\n",
    "Your goal is to, without much structure provided, consider what we discussed in lecture last week and utilize the data provided to answer the question(s) we discussed in class last week: \n",
    "\n",
    "1. Does Congress have an age problem?\n",
    "2. Is this problem exclusive to one of the two major parties?\n",
    "\n",
    "The data you have to start with are available here: [congress-terms.csv](https://github.com/fivethirtyeight/data/tree/master/congress-age). They were used in [this piece](https://fivethirtyeight.com/features/both-republicans-and-democrats-have-an-age-problem/) at FiveThirtyEight. Note, there is an entry for every member of congress who has served at any point during a particular congress between January 1947 and Februrary 2014. One thing to keep in mind is the fact that elections have occurred since 2014 that are not included in this dataset. Getting up-to-date data will be explored in Part III of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Setup & Data\n",
    "\n",
    "In this section you'll want to:\n",
    "- **import any packages** you'll need for your analysis\n",
    "    - Reminder: we'll be doing web scraping, so you'll need to include the following:\n",
    "        - `import requests`\n",
    "        - `import bs4` \n",
    "        - `from bs4 import BeautifulSoup`\n",
    "\n",
    "- **get the data**:\n",
    "    - read the Congress dataset in (URL: https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-age/congress-terms.csv)\n",
    "    - Read, understand, and run the webscraping code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ CONGRESS DATA IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE TO SCRAPE TABLE FROM WIKIPEDIA\n",
    "## UNDERSTAND AND RUN ALL FOLLOWING CELLS\n",
    "\n",
    "# specify webpage we want to scrape \n",
    "wiki = 'https://en.wikipedia.org/wiki/Demography_of_the_United_States'\n",
    "req = requests.get(wiki)\n",
    "soup = BeautifulSoup(req.content) # get contents of web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"wikitable\" style=\"text-align:right\">\n",
       "<tbody><tr>\n",
       "<th>Years</th>\n",
       "<th>1820</th>\n",
       "<th>1830</th>\n",
       "<th>1840</th>\n",
       "<th>1850</th>\n",
       "<th>1860</th>\n",
       "<th>1870</th>\n",
       "<th>1880</th>\n",
       "<th>1890</th>\n",
       "<th>1900\n",
       "</th></tr>\n",
       "<tr>\n",
       "<td align=\"left\">Median age of the total population</td>\n",
       "<td>16.7</td>\n",
       "<td>17.2</td>\n",
       "<td>17.8</td>\n",
       "<td>18.9</td>\n",
       "<td>19.4</td>\n",
       "<td>20.2</td>\n",
       "<td>20.9</td>\n",
       "<td>22.0</td>\n",
       "<td>22.9\n",
       "</td></tr>\n",
       "<tr>\n",
       "<td align=\"left\">Median age of males</td>\n",
       "<td>16.6</td>\n",
       "<td>17.2</td>\n",
       "<td>17.9</td>\n",
       "<td>19.2</td>\n",
       "<td>19.8</td>\n",
       "<td>20.2</td>\n",
       "<td>21.2</td>\n",
       "<td>22.3</td>\n",
       "<td>23.3\n",
       "</td></tr>\n",
       "<tr>\n",
       "<td align=\"left\">Median age of females</td>\n",
       "<td>16.8</td>\n",
       "<td>17.3</td>\n",
       "<td>17.8</td>\n",
       "<td>18.6</td>\n",
       "<td>19.1</td>\n",
       "<td>20.1</td>\n",
       "<td>20.7</td>\n",
       "<td>21.6</td>\n",
       "<td>22.4\n",
       "</td></tr></tbody></table>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitables = soup.findAll(\"table\", 'wikitable') # get tables\n",
    "# extract the tables we want\n",
    "tbl1 = wikitables[5] \n",
    "tbl2 = wikitables[6]\n",
    "tbl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some empty dataframes\n",
    "# note the tables aren't the same size. ugh.\n",
    "new_tbl1 = pd.DataFrame(columns=range(0,10), index = range(0,3)) # I know the size \n",
    "new_tbl2 = pd.DataFrame(columns=range(0,13), index = range(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column names for our first table\n",
    "ind=0\n",
    "cols_list = []\n",
    "for header in tbl1.find_all('tr'): # specify HTML tags\n",
    "    header_name = header.find_all('th') # tag containing column names\n",
    "    for head in header_name:\n",
    "        cols_list.append(head.get_text()) # get the text from between the tags\n",
    "new_tbl1.columns = [s.replace('\\n','') for s in cols_list] # get rid of new line characters in column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the contents for our first table\n",
    "row_marker = -1\n",
    "for row in tbl1.find_all('tr'):\n",
    "    column_marker = 0\n",
    "    columns = row.find_all('td') # different tag than above for table contents\n",
    "    for column in columns:\n",
    "        new_tbl1.iat[row_marker,column_marker] = column.get_text()\n",
    "        column_marker += 1\n",
    "    row_marker += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column names for the second table\n",
    "ind=0\n",
    "cols_list = []\n",
    "for header in tbl2.find_all('tr'):\n",
    "    header_name = header.find_all('th')\n",
    "    for head in header_name:\n",
    "        cols_list.append(head.get_text())\n",
    "new_tbl2.columns = [s.replace('\\n','') for s in cols_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in contents for second table\n",
    "row_marker = -1\n",
    "for row in tbl2.find_all('tr'):\n",
    "    column_marker = 0\n",
    "    columns = row.find_all('td')\n",
    "    for column in columns:\n",
    "        new_tbl2.iat[row_marker,column_marker] = column.get_text()\n",
    "        column_marker += 1\n",
    "    row_marker += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of course there are new line characters to get rid of \n",
    "new_tbl2 = new_tbl2.replace(r'[\\\\n,\\n]',' ', regex=True) \n",
    "new_tbl1 = new_tbl1.replace(r'[\\\\n,\\n]',' ', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Years as index\n",
    "new_tbl1.set_index(['Years'], inplace=True)\n",
    "new_tbl2.set_index(['Years'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get it into tidy data format\n",
    "age = new_tbl1.join(new_tbl2) # join two tables into one by index\n",
    "age = age.transpose() # flip it so observations in rows\n",
    "age = age.reset_index() # add year back in as a column\n",
    "age.columns = ['year', 'age', 'median_males', 'median_females'] # rename columns\n",
    "age = age.apply(pd.to_numeric, errors='coerce') # fix type of information stored\n",
    "age # flipping finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Descriptive & Exploratory Analyses\n",
    "\n",
    "Once you understand your dataset, you'll want to:\n",
    "- **understand** (describe) what's going on in the data\n",
    "    - how many observations?\n",
    "    - what variables do you have? what variable types are here\n",
    "    - which will you need to answer the question?\n",
    "- Carry out **EDA**\n",
    "    - understand the relationships and trends for the variables in your dataset\n",
    "    - generate exploratory visualizations\n",
    "    - answer the exploratory questions we posed in lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTIVE ANALYSIS OF CONGRESS DATASET HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTIVE ANALYSIS OF AGE DATASET HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: More data?\n",
    "\n",
    "Once you have a good handle on what's going on in the dataset, how can you get the data you need to include recent elections? I'll note that in the FiveThirtyEight piece where these data came from, they mention using the NYT's Congress API. However, this has moved to Propublica since publication of that piece: https://projects.propublica.org/api-docs/congress-api/.\n",
    "\n",
    "This will require you to read API documentation and understand it. Additionally, you will likely need additional packages. It may be helpful to consider the following packages: `json`, `requests` & `pandas.io.json`\n",
    "\n",
    "Note that this is optional and will take time to figure it out. Accessing data from an API is not required here, but if you're interested, have at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
